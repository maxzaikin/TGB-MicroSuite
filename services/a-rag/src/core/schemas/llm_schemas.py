# file: services/a-rag/src/core/schemas/llm_schemas.py

"""
Pydantic models for the LLM and RAG API endpoints.

This module defines the data structures for requests and responses, ensuring
clear, validated data contracts between the API and its clients.
"""
from typing import List, Optional, Dict

from pydantic import BaseModel, Field


class RAGRequest(BaseModel):
    """Schema for the incoming user query to the RAG agent."""
    user_query: str = Field(
        ...,
        description="The query or prompt sent by the user.",
        examples=["What is a ZenML pipeline?"],
    )
    user_id: str = Field(
        ...,
        description="A unique identifier for the user to maintain conversation history.",
        examples=["user_12345"],
    )


class RAGResponse(BaseModel):
    """
    Schema for the dual response from the RAG agent.

    This model provides two distinct answers: one generated from the model's
    internal knowledge (standalone) and another augmented by the RAG context,
    allowing for direct comparison of their quality and relevance.
    """
    rag_answer: str = Field(
        ...,
        description="The answer generated using the context retrieved from the vector database."
    )
    llm_answer: str = Field(
        ...,
        description="The answer generated by the LLM without any retrieved RAG context."
    )
    original_query: str = Field(
        ...,
        description="The original user query that initiated the request."
    )
    # In the future, we could add retrieved source documents for traceability.
    # sources: Optional[List[Dict]] = None
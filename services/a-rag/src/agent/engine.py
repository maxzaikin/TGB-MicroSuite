"""
file: services/a-rag/src/agent/engine.py

Core orchestration engine for generating contextual chat responses.

This module contains the primary logic for:
1. Loading the GGUF Language Model using llama-cpp-python.
2. Constructing a structured, role-based prompt (ChatML format).
3. Interacting with the MemoryService to retrieve and update conversation history.
4. Generating a final answer using the loaded model.
"""

import logging
from typing import Dict, List, Optional

from llama_cpp import Llama

from core.config import settings
from core.schemas.chat_schemas import ChatMessage
from memory.service import MemoryService

# --- System Prompt Definition ---
# This defines the persona and instructions for the bot. It's a key part
# of controlling the model's behavior.
SYSTEM_PROMPT = "You are a helpful and friendly assistant named TGBuddy. Your answers should be concise."


def load_llm_model() -> Optional[Llama]:
    """
    Loads the GGUF model from the path specified in the application settings.

    This function initializes the Llama object from llama-cpp-python,
    configuring it with parameters suitable for a chat model.

    Returns:
        An instance of the Llama model if successful, otherwise None.
    """
    # Get the model path from our centralized settings configuration.
    model_path = settings.MODEL_PATH
    logging.info(f"Attempting to load LLM model from path: {model_path}")

    try:
        llm = Llama(
            model_path=str(model_path),
            n_ctx=4096,  # Context window size
            n_gpu_layers=-1,  # Offload all possible layers to GPU
            verbose=False,  # Set to True for detailed Llama.cpp logs
        )
        logging.info("LLM model loaded successfully.")
        return llm
    except Exception:
        # Use logging.exception to automatically include the traceback
        logging.exception(f"Failed to load LLM model from {model_path}")
        return None


def _build_prompt_messages(
    history: List[ChatMessage], user_prompt: str
) -> List[Dict[str, str]]:
    """
    Constructs the full message list for the LLM call in ChatML format.

    Args:
        history: A list of previous ChatMessage objects from memory.
        user_prompt: The current prompt from the user.

    Returns:
        A list of dictionaries, ready for the `create_chat_completion` method.
    """
    messages = [{"role": "system", "content": SYSTEM_PROMPT}]

    # Convert Pydantic models from history into dictionaries
    messages.extend([msg.model_dump() for msg in history])

    messages.append({"role": "user", "content": user_prompt})
    return messages


async def generate_chat_response(
    llm: Llama,
    memory_service: MemoryService,
    user_id: str,
    user_prompt: str,
) -> str:
    """
    Generates a contextual chat response using conversation history from Redis.

    This is the main entry point for the agent's chat logic.

    Args:
        llm: The loaded Llama model instance.
        memory_service: The service for interacting with Redis memory.
        user_id: The unique identifier for the user's session.
        user_prompt: The latest prompt from the user.

    Returns:
        The text generated by the model.
    """
    if llm is None:
        # This case should be handled by the API layer, but we double-check here.
        raise RuntimeError("LLM model is not loaded. Cannot generate text.")

    logging.info(f"Generating response for user '{user_id}'...")

    # 1. Retrieve typed conversation history from Redis
    history = await memory_service.get_history(user_id)
    logging.info(
        f"Retrieved {len(history)} messages from history for user '{user_id}'."
    )

    # 2. Build the structured message list
    messages_for_llm = _build_prompt_messages(history, user_prompt)

    # 3. Call the model using the create_chat_completion method
    output = llm.create_chat_completion(
        messages=messages_for_llm,
        max_tokens=512,  # Limit the length of the response
    )

    generated_text = output["choices"][0]["message"]["content"].strip()
    logging.info(f"LLM generated response: '{generated_text[:100]}...'")

    # 4. Update the history in Redis with the new user message and AI response
    await memory_service.add_message_to_history(
        user_id=user_id, role="user", content=user_prompt
    )

    await memory_service.add_message_to_history(
        user_id=user_id, role="assistant", content=generated_text
    )
    logging.info(f"Updated history for user '{user_id}'.")

    return generated_text
